{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c385014-1da2-4ca0-b825-cf8fed535e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import warnings, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ======== Configuration ========\n",
    "CSV_PATH = \"9_makale_data_guncel.csv\"\n",
    "TARGET   = \"band_gap\"\n",
    "ID_COLS  = [\"material_id\", \"formula_pretty\"]\n",
    "RANDOM_STATE = 42\n",
    "MODEL_PATH = \"xgb_model.pkl\"\n",
    "EARLY_STOP = 100\n",
    "\n",
    "# --- Data Loading and Indexing ---\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df = df.drop(columns=[c for c in ID_COLS if c in df.columns], errors=\"ignore\")\n",
    "y_all = df[TARGET].astype(float)\n",
    "X_all = df.drop(columns=[TARGET]).select_dtypes(include=\"number\")\n",
    "\n",
    "# Clean potential infinite values\n",
    "X_all = X_all.replace([np.inf, -np.inf], np.nan)\n",
    "mask = X_all.notna().all(axis=1) & y_all.notna()\n",
    "X_all, y_all = X_all.loc[mask], y_all.loc[mask]\n",
    "\n",
    "# Load predefined split indices\n",
    "idx_tr = pd.read_csv(\"split_train.csv\")[\"idx\"].values\n",
    "idx_va = pd.read_csv(\"split_val.csv\")[\"idx\"].values\n",
    "idx_te = pd.read_csv(\"split_test.csv\")[\"idx\"].values\n",
    "\n",
    "X_train, y_train = X_all.iloc[idx_tr], y_all.iloc[idx_tr]\n",
    "X_val,   y_val   = X_all.iloc[idx_va], y_all.iloc[idx_va]\n",
    "X_test,  y_test  = X_all.iloc[idx_te], y_all.iloc[idx_te]\n",
    "\n",
    "# --- XGBoost Data Structure (DMatrix) ---\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval   = xgb.DMatrix(X_val,   label=y_val)\n",
    "dtest  = xgb.DMatrix(X_test,  label=y_test)\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"learning_rate\": 0.03865291106813676,\n",
    "    \"max_depth\": 9,\n",
    "    \"min_child_weight\": 11.689593895871,\n",
    "    \"subsample\": 0.8809231752905414,\n",
    "    \"colsample_bytree\": 0.864323016559627,\n",
    "    \"colsample_bylevel\": 0.9986567609362819,\n",
    "    \"gamma\": 0.0015113421306209107,\n",
    "    \"reg_alpha\": 24.332514099721607,\n",
    "    \"reg_lambda\": 3.062237425668688,\n",
    "    \"tree_method\": \"hist\", # Histogram-based algorithm for speed\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"seed\": RANDOM_STATE,\n",
    "    \"nthread\": -1,\n",
    "}\n",
    "\n",
    "num_boost_round = 1336\n",
    "watchlist = [(dtrain, \"train\"), (dval, \"val\")]\n",
    "\n",
    "# Training with Early Stopping\n",
    "booster = xgb.train(\n",
    "    params=params,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=watchlist,\n",
    "    early_stopping_rounds=EARLY_STOP,\n",
    "    verbose_eval=False,\n",
    ")\n",
    "\n",
    "# --- Performance Metrics ---\n",
    "def print_metrics(name, y_true, y_pred):\n",
    "    print(f\"{name:10} -> R²: {r2_score(y_true, y_pred):.6f} | MAE: {mean_absolute_error(y_true, y_pred):.6f}\")\n",
    "\n",
    "# Inference using the best iteration identified during training\n",
    "y_tr_pred = booster.predict(dtrain, iteration_range=(0, booster.best_iteration + 1))\n",
    "y_va_pred = booster.predict(dval,   iteration_range=(0, booster.best_iteration + 1))\n",
    "y_te_pred = booster.predict(dtest,  iteration_range=(0, booster.best_iteration + 1))\n",
    "\n",
    "print(f\"Best iteration: {booster.best_iteration}\")\n",
    "print_metrics(\"Train\", y_train, y_tr_pred)\n",
    "print_metrics(\"Validation\", y_val, y_va_pred)\n",
    "print_metrics(\"Test\", y_test, y_te_pred)\n",
    "\n",
    "# --- Sklearn-compatible Wrapper for Stacking ---\n",
    "class XGBSklearnWrapper:\n",
    "    \"\"\"Wrapper to make the XGBoost Booster compatible with Sklearn-style .predict(X) calls.\"\"\"\n",
    "    def __init__(self, booster, feature_names):\n",
    "        self.booster = booster\n",
    "        self.feature_names_ = list(feature_names)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Ensure column order matches training if X is a DataFrame\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X[self.feature_names_]\n",
    "        dm = xgb.DMatrix(X)\n",
    "        return self.booster.predict(dm, iteration_range=(0, self.booster.best_iteration + 1))\n",
    "\n",
    "# Initialize wrapper and save\n",
    "final_model = XGBSklearnWrapper(booster, X_train.columns)\n",
    "with open(MODEL_PATH, \"wb\") as f:\n",
    "    pickle.dump(final_model, f)\n",
    "\n",
    "print(f\"Saved: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9a8067-c360-448c-be8a-6f99bb430131",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save\n",
    "wrapper = XGBSklearnWrapper(booster, X_all.columns)\n",
    "with open(MODEL_PATH, \"wb\") as f:\n",
    "    pickle.dump(wrapper, f)\n",
    "\n",
    "print(f\"saved: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb4dce3-fdaf-44c5-a5b8-784a9ce2c462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# === 1. SHAP Interpretability (Beeswarm Plot) ===\n",
    "print(\"\\nCalculating SHAP values...\")\n",
    "\n",
    "# Use a representative subset for background and calculation to save time\n",
    "rs = np.random.RandomState(42)\n",
    "bg_idx   = rs.choice(len(X_train), size=min(500, len(X_train)), replace=False)\n",
    "shap_idx = rs.choice(len(X_test),  size=min(2000, len(X_test)), replace=False)\n",
    "X_bg, X_sh = X_train.iloc[bg_idx], X_test.iloc[shap_idx]\n",
    "\n",
    "try:\n",
    "    # Attempt using the optimized TreeExplainer for XGBoost\n",
    "    explainer = shap.TreeExplainer(booster)\n",
    "    shap_vals = explainer.shap_values(X_sh)\n",
    "except Exception as e:\n",
    "    print(\"TreeExplainer failed, falling back to KernelExplainer:\", e)\n",
    "    # Use KernelExplainer as a backup (slower but model-agnostic)\n",
    "    explainer = shap.KernelExplainer(wrapper.predict, X_bg)\n",
    "    shap_vals = explainer.shap_values(X_sh, nsamples=\"auto\")\n",
    "\n",
    "# Generate and save SHAP summary plot\n",
    "\n",
    "shap.summary_plot(shap_vals, X_sh, show=False, plot_type=\"dot\")\n",
    "plt.title(\"SHAP Feature Importance (Test Set)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"shap_summary_test.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# === 2. Predicted vs. True Plot (Parity Plot) ===\n",
    "print(\"Generating Predicted vs. True plot...\")\n",
    "\n",
    "y_pred_test = y_te_pred  # Using the test predictions calculated previously\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.scatterplot(x=y_test, y=y_pred_test, alpha=0.6, edgecolor=None)\n",
    "mn, mx = float(y_test.min()), float(y_test.max())\n",
    "plt.plot([mn, mx], [mn, mx], 'r--', label=\"Ideal (y=x)\")\n",
    "plt.xlabel(\"True Band Gap (eV)\")\n",
    "plt.ylabel(\"Predicted Band Gap (eV)\")\n",
    "plt.title(\"Actual vs. Predicted Performance\")\n",
    "plt.legend(); plt.grid(True); plt.tight_layout()\n",
    "plt.savefig(\"predicted_vs_true_test.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# === 3. Residuals vs. Predicted Plot ===\n",
    "print(\"Generating Residuals plot...\")\n",
    "\n",
    "residuals = y_test - y_pred_test\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.scatterplot(x=y_pred_test, y=residuals, alpha=0.6, edgecolor=None)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel(\"Predicted Band Gap (eV)\")\n",
    "plt.ylabel(\"Residuals (Error)\")\n",
    "plt.title(\"Residual Analysis\")\n",
    "plt.grid(True); plt.tight_layout()\n",
    "plt.savefig(\"residuals_test.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"✅ Visualizations saved: shap_summary_test.png, predicted_vs_true_test.png, residuals_test.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
